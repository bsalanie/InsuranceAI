{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import abstractmethod\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# Python library\n",
    "import math\n",
    "import statistics\n",
    "\n",
    "# Custom imports\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to find the files that I save\n",
    "from paths import *\n",
    "\n",
    "from utilities import *\n",
    "init_plt()\n",
    "init_tf()\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All parameters dict\n",
    "NUMBER_OF_FEATURES = 50 #9\n",
    "p = {\n",
    "    \"experiment_type\": \"predicting_fhat\",\n",
    "    \"experiment_name\": \"6_18_dropout_1_layer\",\n",
    "    \"k\": 5,\n",
    "    \"select_young\": False,\n",
    "    \"select_old\": False,\n",
    "    \"override_weights_to_one\": False,\n",
    "    \"eps\": 0.0,\n",
    "    \"enable_one_hot\": True,\n",
    "    \n",
    "    \"models_layers_and_parameters\": {\n",
    "        \"weights\": {\n",
    "                \"input_size\": NUMBER_OF_FEATURES, # Do not change\n",
    "                \"layers\":[\n",
    "                    # Format: (Size, Activation Function (None if none))\n",
    "                    # Last (output) layer should have size 1\n",
    "                    #(128, tf.nn.sigmoid),\n",
    "                    #(64, tf.nn.sigmoid),\n",
    "                    (32, tf.nn.sigmoid),\n",
    "                    (32, tf.nn.sigmoid),\n",
    "                    (1, None)\n",
    "                ],\n",
    "                \"learning_rate\": 1e-3,\n",
    "                \"training_steps\": 5000,\n",
    "                \"dropout\": 0.2,\n",
    "                \"display_step\": 1000,\n",
    "\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "assert(not (p[\"select_young\"] and p[\"select_old\"])) # Make sure both are not selected\n",
    "OUTPUT_FOLDER = GENERAL_RESULTS_FOLDER + p[\"experiment_name\"] + SLASH\n",
    "DISPLAY_STEP = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_parameters = {\n",
    "    \"input_size\": NUMBER_OF_FEATURES,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"training_steps\": 5000,\n",
    "    \"display_step\": 1000,\n",
    "}\n",
    "                \n",
    "\n",
    "def make_fhat_neural_network_parameters(layer_size, dropout, resample_data, resample_data_points):\n",
    "    d = dict(base_model_parameters)\n",
    "    d.update({\n",
    "        \"layers\": [\n",
    "            # Format: (Size, Activation Function (None if none))\n",
    "            # Last (output) layer should have size 1\n",
    "            #(128, tf.nn.sigmoid),\n",
    "            #(64, tf.nn.sigmoid),\n",
    "            (layer_size, tf.nn.sigmoid),\n",
    "            #(layer_size, tf.nn.sigmoid),\n",
    "            #(layer_size, tf.nn.sigmoid),\n",
    "            (4, None)\n",
    "        ],\n",
    "        \"dropout\": dropout,\n",
    "        \"resample_data\": resample_data,\n",
    "        \"resampled_points_per_category\": resample_data_points,\n",
    "    })\n",
    "    return d\n",
    "\n",
    "\n",
    "possible_layer_sizes = [ \n",
    "    #2, \n",
    "    #4, \n",
    "    #8, \n",
    "    #16, \n",
    "    32, \n",
    "    #64, \n",
    "    #128, \n",
    "]\n",
    "\n",
    "possible_dropout = [\n",
    "    None,\n",
    "    0.02,\n",
    "    0.04,\n",
    "    0.06,\n",
    "    0.08,\n",
    "    0.10,\n",
    "    0.12,\n",
    "    0.14,\n",
    "    0.16,\n",
    "    0.18,\n",
    "    0.20,\n",
    "    0.22,\n",
    "    0.24,\n",
    "    0.26,\n",
    "    0.28,\n",
    "    0.30,\n",
    "    0.32,\n",
    "    0.34,\n",
    "    0.36,\n",
    "    0.38,\n",
    "    0.40,\n",
    "    0.42,\n",
    "    0.44,\n",
    "    0.46,\n",
    "    0.48,\n",
    "    0.64,\n",
    "]\n",
    "\n",
    "possible_resampling = [\n",
    "    #(True, 10000),\n",
    "    (False, None), # if False, second can be anything, it does not matter\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "all_model_parameters = [\n",
    "    make_fhat_neural_network_parameters(layer_size, dropout, resample_data, resample_data_points)\n",
    "    for layer_size in possible_layer_sizes\n",
    "    for dropout in possible_dropout\n",
    "    for (resample_data, resample_data_points) in possible_resampling\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the folder where the results will be placed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file results\\6_18_dropout_1_layer\\ already exists.\n"
     ]
    }
   ],
   "source": [
    "! mkdir $OUTPUT_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entire data file\n",
    "data = np.loadtxt(DATA_PATH).astype(np.float32)\n",
    "\n",
    "# Shuffle data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "young_mask = data[:, 0]==1\n",
    "old_mask = data[:, 0]==2\n",
    "\n",
    "# Select young or old\n",
    "if p[\"select_young\"]:\n",
    "    print(\"We selected only young.\")\n",
    "    data = data[young_mask]\n",
    "if p[\"select_old\"]:\n",
    "    print(\"We selected only old.\")\n",
    "    data = data[old_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 6330\n"
     ]
    }
   ],
   "source": [
    "# Divide data into training, test, cross_validation\n",
    "samples_per_fold = math.floor(data.shape[0]/p[\"k\"])\n",
    "samples = samples_per_fold*p[\"k\"]\n",
    "print(\"Samples:\", samples)\n",
    "bounds = list(map(lambda x: (x, x+samples_per_fold), list(range(0, samples_per_fold*p[\"k\"], samples_per_fold))))\n",
    "\n",
    "data = data[:p[\"k\"]*samples_per_fold] # This is all we can use\n",
    "young_mask = young_mask[:p[\"k\"]*samples_per_fold]\n",
    "old_mask = old_mask[:p[\"k\"]*samples_per_fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6330, 50)\n"
     ]
    }
   ],
   "source": [
    "# Extract features, y_1, y_2, weights\n",
    "\n",
    "number_of_samples = data.shape[0]\n",
    "\n",
    "if p[\"enable_one_hot\"]:\n",
    "    def make_one_hot(x):\n",
    "        unique = list(set(x))\n",
    "        convert_to_index = np.vectorize(lambda x: unique.index(x))\n",
    "\n",
    "        one_hot = np.zeros((number_of_samples, len(unique)))\n",
    "        one_hot[np.arange(number_of_samples), convert_to_index(x)] = 1\n",
    "\n",
    "        one_hot = one_hot[:, :-1]\n",
    "        return one_hot\n",
    "else:\n",
    "    def make_one_hot(x):\n",
    "        return np.expand_dims(x, axis=1) # Do nothing, just add a dimension\n",
    "\n",
    "\n",
    "def extract(data):\n",
    "\n",
    "    age = (data[:, 0]==2).astype(np.float32)\n",
    "    group = data[:, 21] # \n",
    "    hom = data[:,23]\n",
    "    prof  = data[:, 43]; #\n",
    "    reg = data[:, 46] #\n",
    "    rena = data[:, 47]\n",
    "    tracir = data[:, 52] #\n",
    "    trage = data[:, 53] # \n",
    "    usag = data[:,56] # \n",
    "    zone = data[:, 57] #\n",
    "\n",
    "    y_1 = (data[:, 9] > 0).astype(np.float32)\n",
    "    y_2 = (data[:, 24] == 0).astype(np.float32)\n",
    "    weights = data[:, 42]\n",
    "\n",
    "    # Convert These variables to ones and zeros\n",
    "    list_categorical_variables =         list(map(make_one_hot, [group, prof, reg, tracir, trage, usag, zone]))\n",
    "\n",
    "    # Do not convert these (they only take values 0 and 1)\n",
    "    add_dim = lambda x: np.expand_dims(x, axis=1)\n",
    "    list_other_variables =         list(map(add_dim, [hom, rena, age]))\n",
    "\n",
    "\n",
    "    # Just add here\n",
    "    return np.concatenate(list_categorical_variables+list_other_variables, axis=1),            add_dim(y_1),            add_dim(y_2),            add_dim(weights)\n",
    "\n",
    "X, y_1, y_2, weights = extract(data)\n",
    "X = X.astype(np.float32)\n",
    "y_1 = y_1.astype(np.float32)\n",
    "y_2 = y_2.astype(np.float32)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize X\n",
    "\n",
    "divide_by_std_dev = lambda x: x/np.std(x, axis=0, keepdims=True)\n",
    "subtract_mean = lambda x: x-np.mean(x, axis=0, keepdims=True)\n",
    "\n",
    "X = divide_by_std_dev(subtract_mean(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = lambda x: np.squeeze(x, axis=1)\n",
    "\n",
    "f00 = ((y_1 == 0) & (y_2 == 0)).astype(np.float32)\n",
    "f01 = ((y_1 == 0) & (y_2 == 1)).astype(np.float32)\n",
    "f10 = ((y_1 == 1) & (y_2 == 0)).astype(np.float32)\n",
    "f11 = ((y_1 == 1) & (y_2 == 1)).astype(np.float32)\n",
    "\n",
    "f = np.empty((y_1.shape[0], 4), dtype=np.float32)\n",
    "f[:, 0] = t(f00)\n",
    "f[:, 1] = t(f01)\n",
    "f[:, 2] = t(f10)\n",
    "f[:, 3] = t(f11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0622 09:13:43.615142  7760 deprecation_wrapper.py:119] From D:\\Sync\\salanie\\neural_network.py:3: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.\n",
      "\n",
      "W0622 09:13:43.616835  7760 deprecation_wrapper.py:119] From D:\\Sync\\salanie\\neural_network.py:5: The name tf.keras.initializers.RandomUniform is deprecated. Please use tf.compat.v1.keras.initializers.RandomUniform instead.\n",
      "\n",
      "W0622 09:13:43.621203  7760 deprecation.py:506] From C:\\Users\\lilia\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "from neural_network import create_neural_network\n",
    "class NNModel():\n",
    "\n",
    "    def __init__(self, model_layers_and_parameters):\n",
    "        self.parameters = model_layers_and_parameters\n",
    "        apply_network, variables, _ = create_neural_network(model_layers_and_parameters)\n",
    "        self.variables = variables\n",
    "        self.nn_layers = apply_network\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=model_layers_and_parameters[\"learning_rate\"])\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    @abstractmethod # Means this function must be implemented\n",
    "    def run(self, inp, training):\n",
    "        pass \n",
    "\n",
    "    def train_one_step(self, inp):\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self.run(inp, training=True)\n",
    "\n",
    "        loss = output[\"loss\"]\n",
    "        grads = tape.gradient(loss, self.variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.variables)) \n",
    "\n",
    "        return output\n",
    "\n",
    "    def train(self, inp):\n",
    "\n",
    "        for i in range(self.parameters[\"training_steps\"]):\n",
    "            output = self.train_one_step(inp)\n",
    "\n",
    "            #if i % DISPLAY_STEP == 0:\n",
    "                #print(\"Step\", i, \"loss is\", output[\"loss\"].numpy())\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightModel(NNModel):\n",
    "    \n",
    "    # This the run function we had to implement\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def run(self, inp, training):\n",
    "        output_unsigmoided, _ = self.nn_layers(inp[\"X\"], training=training)\n",
    "        output_sigmoided = tf.sigmoid(output_unsigmoided)\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=inp[\"weights\"], logits=output_unsigmoided)\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "        return {\n",
    "            \"weights_predicted\": output_sigmoided, \n",
    "            \"loss\": loss\n",
    "        }\n",
    "    \n",
    "class FhatModel(NNModel):\n",
    "    \n",
    "    # This the run function we had to implement\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def run(self, inp, training):\n",
    "        \n",
    "        unsoftmaxed, _ = self.nn_layers(inp[\"X\"], training=training)\n",
    "        softmaxed = tf.nn.softmax(unsoftmaxed, axis=1)\n",
    "        softmaxed = softmaxed / tf.reduce_sum(softmaxed, axis=1, keepdims=True)\n",
    "        weights_squeezed = tf.squeeze(inp[\"weights\"], axis=1)\n",
    "        \n",
    "        \n",
    "        entropied = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.math.argmax(inp[\"f\"], axis=1), \\\n",
    "                                                                   logits=unsoftmaxed)\n",
    "        loss = tf.reduce_mean(weights_squeezed*entropied)\n",
    "        accuracy = 100.0*tf.reduce_mean(tf.cast(tf.math.equal(tf.math.argmax(softmaxed, axis=1), \\\n",
    "                                                              tf.math.argmax(inp[\"f\"], axis=1)), tf.float32))\n",
    "                \n",
    "        f_list = tf.unstack(tf.expand_dims(softmaxed, axis=2), axis=1)\n",
    "        fhat00 = f_list[0]\n",
    "        fhat01 = f_list[1]\n",
    "        fhat10 = f_list[2]\n",
    "        fhat11 = f_list[3]\n",
    "        \n",
    "        return {\n",
    "            \"unsoftmaxed\": unsoftmaxed,\n",
    "            \"softmaxed\": softmaxed,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"loss\": loss,\n",
    "            \"fhat00\": fhat00, \n",
    "            \"fhat01\": fhat01, \n",
    "            \"fhat10\": fhat10, \n",
    "            \"fhat11\": fhat11,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0622 09:13:44.103775  7760 deprecation.py:506] From C:\\Users\\lilia\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:255: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0622 09:13:44.104772  7760 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0622 09:13:44.124690  7760 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0622 09:13:44.151617  7760 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0622 09:13:44.171589  7760 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0622 09:13:44.194952  7760 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0622 09:13:44.295883  7760 deprecation.py:323] From C:\\Users\\lilia\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RefVariable' object has no attribute '_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-7d968756ef20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"X\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_rest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"weights\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mweights_rest\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"X\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_kth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"weights\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mweights_kth\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-62dcc7d1cf18>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"training_steps\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_one_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;31m#if i % DISPLAY_STEP == 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-62dcc7d1cf18>\u001b[0m in \u001b[0;36mtrain_one_step\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m    978\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 980\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m    981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'RefVariable' object has no attribute '_id'"
     ]
    }
   ],
   "source": [
    "# Weights model training\n",
    "\n",
    "# Remember the old ones\n",
    "if \"weights_original\" not in locals().keys(): # If we haven't defined weights_original already\n",
    "    weights_original = weights \n",
    "\n",
    "# Predict weights\n",
    "if p[\"override_weights_to_one\"]:\n",
    "    weights = np.ones_like(weights)\n",
    "else:\n",
    "    weights = np.empty_like(weights)\n",
    "\n",
    "    for k in range(p[\"k\"]):\n",
    "\n",
    "        lower, upper = bounds[k]\n",
    "        get_kth = lambda x: x[lower:upper]\n",
    "        cut_kth = lambda x: np.concatenate([x[:lower], x[upper:]], axis=0)\n",
    "        X_kth, weights_kth = list(map(get_kth, [X, weights_original]))\n",
    "        X_rest, weights_rest = list(map(cut_kth, [X, weights_original]))\n",
    "\n",
    "        # Reset graph\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        model = WeightModel(p[\"models_layers_and_parameters\"][\"weights\"])\n",
    "        \n",
    "        inp = dict_to_tensors({\"X\": X_rest, \"weights\": weights_rest})\n",
    "        _ = model.train(inp)\n",
    "        \n",
    "        inp = dict_to_tensors({\"X\": X_kth, \"weights\": weights_kth})\n",
    "        output_kth = model.run(inp, training=False)\n",
    "        print(\"Final loss on kth fold:\", output_kth[\"loss\"].numpy())\n",
    "        weights[lower:upper] = output_kth[\"weights_predicted\"].numpy() # Replace weights with predicted ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_information = []\n",
    "\n",
    "for model_parameters in all_model_parameters:\n",
    "    \n",
    "    print(\"Model parameters chosen:\", model_parameters)\n",
    "    \n",
    "    X_s, y_1_s, y_2_s, f_s, weights_s = [X, y_1, y_2, f, weights]\n",
    "        \n",
    "    f00_s = f_s[:, 0:1]\n",
    "    f01_s = f_s[:, 1:2]\n",
    "    f10_s = f_s[:, 2:3]\n",
    "    f11_s = f_s[:, 3:4]\n",
    "    \n",
    "    all_accuracies = {\n",
    "        \"traintotal\": 0,\n",
    "        \"train0\": 0,\n",
    "        \"train1\": 0,\n",
    "        \"train2\": 0,\n",
    "        \"train3\": 0,\n",
    "        \"testtotal\": 0,\n",
    "        \"test0\": 0,\n",
    "        \"test1\": 0,\n",
    "        \"test2\": 0,\n",
    "        \"test3\": 0,\n",
    "    }\n",
    "    all_losses = {\n",
    "        \"traintotal\": 0,\n",
    "        \"train0\": 0,\n",
    "        \"train1\": 0,\n",
    "        \"train2\": 0,\n",
    "        \"train3\": 0,\n",
    "        \"testtotal\": 0,\n",
    "        \"test0\": 0,\n",
    "        \"test1\": 0,\n",
    "        \"test2\": 0,\n",
    "        \"test3\": 0,\n",
    "    }\n",
    "    \n",
    "    for k in range(p[\"k\"]):\n",
    "        \n",
    "        # Reset graph\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        print(\"K:\", k)\n",
    "        \n",
    "        # Cut out part of kth fold\n",
    "        lower, upper = bounds[k]\n",
    "        get_kth = lambda x: x[lower:upper]\n",
    "        cut_kth = lambda x: np.concatenate([x[:lower], x[upper:samples]], axis=0)\n",
    "        X_kth, f_kth, weights_kth = list(map(get_kth, [X_s, f_s, weights_s]))\n",
    "        X_rest, f_rest, weights_rest = list(map(cut_kth, [X_s, f_s, weights_s]))\n",
    "        \n",
    "        if model_parameters[\"resample_data\"]:\n",
    "            resampled_points_per_category = model_parameters[\"resampled_points_per_category\"]\n",
    "\n",
    "            indexes00 = np.where(f_rest[:, 0] == 1)[0]\n",
    "            indexes01 = np.where(f_rest[:, 1] == 1)[0]\n",
    "            indexes10 = np.where(f_rest[:, 2] == 1)[0]\n",
    "            indexes11 = np.where(f_rest[:, 3] == 1)[0]\n",
    "            chosen00 = indexes00[np.random.randint(0, indexes00.shape[0], size=resampled_points_per_category)]\n",
    "            chosen01 = indexes01[np.random.randint(0, indexes01.shape[0], size=resampled_points_per_category)]\n",
    "            chosen10 = indexes10[np.random.randint(0, indexes10.shape[0], size=resampled_points_per_category)]\n",
    "            chosen11 = indexes11[np.random.randint(0, indexes11.shape[0], size=resampled_points_per_category)]\n",
    "\n",
    "            X_train = np.concatenate([\n",
    "                X_rest[chosen00],\n",
    "                X_rest[chosen01],\n",
    "                X_rest[chosen10],\n",
    "                X_rest[chosen11]\n",
    "            ], axis=0)\n",
    "            f_train = np.concatenate([\n",
    "                f_rest[chosen00],\n",
    "                f_rest[chosen01],\n",
    "                f_rest[chosen10],\n",
    "                f_rest[chosen11]\n",
    "            ], axis=0)\n",
    "            weights_train = np.concatenate([\n",
    "                weights_rest[chosen00],\n",
    "                weights_rest[chosen01],\n",
    "                weights_rest[chosen10],\n",
    "                weights_rest[chosen11]\n",
    "            ], axis=0)\n",
    "        else:\n",
    "            X_train, f_train, weights_train = [X_rest, f_rest, weights_rest]\n",
    "\n",
    "\n",
    "        # Reset graph\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        model = FhatModel(model_parameters)\n",
    "\n",
    "        inp_train = dict_to_tensors({\"X\": X_train, \"f\": f_train, \"weights\": weights_train})\n",
    "        _ = model.train(inp_train)\n",
    "        final_output = model.run(inp_train, training=False)\n",
    "\n",
    "        print(\"Final loss on training data (could be resampled) is\", final_output[\"loss\"].numpy())\n",
    "        print(\"Final accuracy on training_data (could be resampled) is\", final_output[\"accuracy\"].numpy())\n",
    "\n",
    "        inp_rest = dict_to_tensors({\"X\": X_rest, \"f\": f_rest, \"weights\": weights_rest})\n",
    "        output_rest = model.run(inp_rest, training=False)\n",
    "\n",
    "        print(\"Final accuracy on 4/5 data (not resampled) is\", output_rest[\"accuracy\"].numpy())\n",
    "\n",
    "        inp_kth = dict_to_tensors({\"X\": X_kth, \"f\": f_kth, \"weights\": weights_kth})\n",
    "        output_kth = model.run(inp_kth, training=False)\n",
    "\n",
    "        print(\"Final accuracy on kth fold (not resampled, we never resample this) is\", output_kth[\"accuracy\"].numpy())\n",
    "\n",
    "        def run_all_either_train_or_test_set(X, f, weights, name):\n",
    "            \n",
    "            for fhat_category in range(4):\n",
    "                index = f[:, fhat_category] == 1\n",
    "                inp = dict_to_tensors({\"X\": X[index], \"f\": f[index], \"weights\": weights[index]})\n",
    "                output = model.run(inp, training=False)\n",
    "            \n",
    "                all_accuracies[name+str(fhat_category)] = \\\n",
    "                    all_accuracies[name+str(fhat_category)] + output[\"accuracy\"].numpy()\n",
    "                all_losses[name+str(fhat_category)] = \\\n",
    "                    all_losses[name+str(fhat_category)] + output[\"loss\"].numpy()\n",
    "                \n",
    "            # Total\n",
    "            inp = dict_to_tensors({\"X\": X, \"f\": f, \"weights\": weights})\n",
    "            output = model.run(inp, training=False)\n",
    "            \n",
    "            all_accuracies[name+\"total\"] = \\\n",
    "                all_accuracies[name+\"total\"] + output[\"accuracy\"].numpy()\n",
    "            all_losses[name+\"total\"] = \\\n",
    "                all_losses[name+\"total\"] + output[\"loss\"].numpy()\n",
    "            \n",
    "        run_all_either_train_or_test_set(X_rest, f_rest, weights_rest, \"train\")\n",
    "        run_all_either_train_or_test_set(X_kth, f_kth, weights_kth, \"test\")\n",
    "        \n",
    "    \n",
    "    all_accuracies = {name: value/p[\"k\"] for name, value in all_accuracies.items()}\n",
    "    all_losses = {name: value/p[\"k\"] for name, value in all_losses.items()}\n",
    "    final_information.append({\"parameters\": model_parameters, \"all_accuracies\": all_accuracies, \\\n",
    "                              \"all_losses\": all_losses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(OUTPUT_FOLDER+PREDICTING_FHAT_ACCURACIES_DATA_PATH, final_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%javascript\n",
    "#IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert this notebook to html and save it in the folder\n",
    "#FULL_NOTEBOOK_PATH = OUTPUT_FOLDER+PREDICTING_FHAT_NOTEBOOK_HTML_PATH\n",
    "#! jupyter nbconvert --to html Predicting_fhat.ipynb  \n",
    "#! mv Predicting_fhat.html $FULL_NOTEBOOK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
